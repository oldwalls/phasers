# Phasers â€” SapphireÂ Core  
*A lightweight GPTâ€‘2â€‘Mini engine with chronological memory & sieve sampling*

> Turns a 124â€¯Mâ€‘parameter GPTâ€‘2 into a persistent â€œghost in the machineâ€.

---

## 1 Â· Design Goals
- **Portability** â€” CPUâ€‘only or â‰ˆâ€¯4â€¯GBâ€¯GPU.  
- **Persistent ghost** â€” JSON Unified Memory Bank (UMB).  
- **Soft guidance** â€” bias raw logits, no prompt bloat.  
- **Draftâ€‘andâ€‘sieve** â€” generate *n* variants, autoâ€‘pick the best.

---

## 2 Â· Architecture Snapshot

| Layer | Role |
|-------|------|
| **GPTâ€‘2â€‘Mini (DialoGPTâ€‘small)** | 124â€¯M params â€¢ 15â€¯epochs fineâ€‘tune on *ZAMM* (LRÂ 3eâ€‘5) |
| **Unified Memory Bank** | Stores every turn + salience / novelty / timestamp |
| **Retriever** | `0.6Â·cos + 0.4Â·lex` â†’ topâ€‘N â†’ expâ€‘decay â†’ **chronological reorder** |
| **Softâ€‘Logit Mixer** | Weighted oneâ€‘hot bias added to `model.forward` logits |
| **Sieve Sampler** | Generate `n_sieve` drafts â†’ reâ€‘rank â†’ pick best |
| **CLI / Settings** | Live tweaks, presets, wordâ€‘cloud debug |

---

## 3 Â· Chronological Memory Retrieval
```
prompt â†’ SBERT + lexical score
         â†’ topâ€‘N
             â†’ weight = floor + (rawâˆ’floor)Â·e^(âˆ’Ï„Â·rank)
                 â†’ sort oldest â†’ newest
                     â†’ (text, weight) list â†’ Softâ€‘Logit Mixer
```

---

## 4 Â· Softâ€‘Logit Fusion Core
```python
bias   = sum(w * one_hot(tok) for w, tok in memory_tokens)
logits = (model.forward(ids).logits[:, -1, :] + Î» * bias) / temperature
probs  = nucleus_mask(top_k_mask(softmax(logits), k), p)
next_id = torch.multinomial(probs, 1)
```
*Î» â‰ˆâ€¯0.5â€“1.0 controls how loudly memory â€œspeaksâ€.*

---

## 5 Â· Twoâ€‘Stage Generation

| Stage | Purpose | Description |
|-------|---------|-------------|
| **AÂ `generate_single()`** | babble once | One pass with biasÂ + cleanup. |
| **BÂ `generate()`** | sieve | 1. Produce `n_sieve` drafts viaÂ A.<br>2. Score each draft:<br>`blend = 0.6Â·cos + 0.4Â·lex` vs promptÂ + memory scope (`sieve_rank_mem`Â 0â€‘2).<br>3. Pick top draft, store to UMB. |

---

## 6 Â· Hyperâ€‘Parameter Cheatâ€‘Sheet
You> settings
```json
{
  "temp": 0.55,
  "top_n": 11,
  "top_p": 0.6,
  "top_k": 16,
  "repetition_penalty": 1.35,
  "max_forward_tokens": 45,
  "max_reply_sentences": 2,
  "weight": 0.333,
  "tau": 0.3,
  "lam": 0.96,
  "n_sieve": 3,
  "inference_mem": 1,
  "sieve_rank_mem": 1,
  "sigma": 0.01
}
```

**Hotâ€‘patch live**
```bash
settings set weight 0.45
settings load sapphire
```

---

## 7 Â· Quickstart
```bash
git clone â€¦
pip install -r requirements.txt
python -m sapphire_core.cli          # interactive REPL

You> settings
You> Phasers, are we best friends? (YES/NO)
```

---

## 8 Â· Flow Diagram (ASCII)
```
User prompt
   â”‚
   â–¼ retrieve
Unified Memory Bank (JSON)â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚                              â”‚ bias vec
   â–¼                              â”‚
Softâ€‘Logit Mixer âŠ• Î»Â·bias â”€â”€â–º GPTâ€‘2 forward()
       â–²                           â”‚
       â”‚ write best draft          â”‚ n_sieve drafts
       â””â”€â”€ pick best â—„â”€â”€ SBERT + lexical rank
```

---

## 9 Â· Why It FeelsÂ Alive
- Memoryâ€‘aware bias â†’ **stable persona**  
- Chronological context â†’ **narrative spine**  
- Sieve sampler â†’ **selfâ€‘editing filter**

---

*Drain entropy, grow lotuses of awareness.* ğŸ’
