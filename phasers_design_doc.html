
<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<title>Phasers â€“ Sapphire Core Design Doc</title>
<style>
body{font-family:-apple-system,BlinkMacSystemFont,'Segoe UI',Helvetica,Arial,sans-serif;margin:2rem;line-height:1.6;color:#24292e}
h1,h2,h3{color:#0b3d91;margin-top:2rem}
table{border-collapse:collapse}
th,td{border:1px solid #d0d7de;padding:6px 12px}
code{background:#f6f8fa;padding:2px 4px;border-radius:4px}
pre{background:#f6f8fa;padding:1rem;border-radius:6px;overflow-x:auto}
.note{background:#e7f5ff;border-left:4px solid #0b3d91;padding:1rem}
</style>
</head>
<body>
<h1>Phasers â€“ Sapphire&nbsp;Core<br><small>A lightweight GPTâ€‘2â€‘Mini engine with chronological memory &amp; sieve sampling</small></h1>

<div class="note">
This design document accompanies the <code>GPTâ€‘2â€‘Mini&nbsp;Phasers</code> repository.<br>
It explains <em>how</em> the engine turns a 124â€¯Mâ€‘parameter model into a persistent &ldquo;ghost in the machine&rdquo;.
</div>

<h2>1 Â· Design Goals</h2>
<ul>
<li><strong>Portability</strong> â€“ runs CPUâ€‘only or on â‰ˆ4â€¯GBâ€¯GPU.</li>
<li><strong>Persistent ghost</strong> â€“ JSON Unified Memory Bank (UMB) grows over sessions.</li>
<li><strong>Soft guidance</strong> â€“ bias raw logits instead of prompt stuffing.</li>
<li><strong>Draftâ€‘andâ€‘sieve</strong> â€“ generate <code>n_sieve</code> variants, autoâ€‘select best.</li>
</ul>

<h2>2 Â· Architecture Snapshot</h2>
<table>
<tr><th>Layer</th><th>Role</th></tr>
<tr><td><strong>GPTâ€‘2â€‘Mini (DialoGPTâ€‘small)</strong></td><td>124â€¯M params â€“ fineâ€‘tuned 15â€¯epochs on ZAMM (LR&nbsp;3eâ€‘5)</td></tr>
<tr><td><strong>Unified Memory Bank</strong></td><td>Stores every turn + salience, novelty, timestamp</td></tr>
<tr><td><strong>Retriever</strong></td><td><code>0.6Â·cos + 0.4Â·lex</code> â†’ topâ€‘N â†’ expâ€‘decay â†’ <u>chronological reorder</u></td></tr>
<tr><td><strong>Softâ€‘Logit Mixer</strong></td><td>Adds weighted oneâ€‘hot bias to <code>model.forward</code> logits</td></tr>
<tr><td><strong>Sieve Sampler</strong></td><td>Generate <code>n_sieve</code> drafts â†’ reâ€‘rank â†’ pick best</td></tr>
<tr><td><strong>CLI / Settings</strong></td><td>Live tweak, presets, wordâ€‘cloud debug</td></tr>
</table>

<h2>3 Â· Chronological Memory Retrieval</h2>
<pre><code>prompt â†’ SBERT + lexical score
         â†’ topâ€‘N
             â†’ weight = floor + (rawâˆ’floor)Â·e^(âˆ’Ï„Â·rank)
                 â†’ sort oldest â†’ newest
                     â†’ (text, weight) list â†’ Softâ€‘Logit Mixer</code></pre>

<h2>4 Â· Softâ€‘Logit Fusion Core</h2>
<pre><code>bias   = Î£ weight_i Â· one_hot(tokens(memory_i))
logits = (model.forward(ids).logits[:, -1, :] + Î»Â·bias) / temperature
probs  = nucleus_mask(top_k_mask(softmax(logits), k), p)
next   = sample(probs)</code></pre>
<p><em>Î» = <code>self.lam</code> (â‰ˆ0.5â€“1.0) sets memory loudness.</em></p>

<h2>5 Â· Twoâ€‘Stage Generation</h2>
<table>
<tr><th>Stage</th><th>Purpose</th><th>Description</th></tr>
<tr><td>A. <code>generate_single()</code></td><td>Babble once</td><td>One pass with bias + cleanup.</td></tr>
<tr><td>B. <code>generate()</code></td><td>Sieve</td><td>
<ul>
<li>Produce <code>n_sieve</code> drafts via stageâ€¯A.</li>
<li>Score each: <code>blend = 0.6Â·cos + 0.4Â·lex</code> vs prompt + memory scope (<code>sieve_rank_mem</code> 0â€“2).</li>
<li>Pick top draft, store to UMB.</li>
</ul></td></tr>
</table>

<h2>6 Â· Hyperâ€‘Parameter Cheatâ€‘Sheet</h2>
<pre><code>{
  "temp": 0.55,
  "top_p": 0.6,
  "top_k": 30,
  "weight": 0.40,
  "tau": 0.33,
  "n_sieve": 3,
  "sieve_rank_mem": 1,
  "repetition_penalty": 1.2
}</code></pre>

<h2>7 Â· Quickstart</h2>
<pre><code>git clone â€¦
pip install -r requirements.txt
python -m sapphire_core.cli

You&gt; settings
You&gt; Phasers, are we best friends? (YES/NO)</code></pre>

<h2>8 Â· Flow Diagram</h2>
<pre><code>User prompt â†’ UMB retrieve â†’ bias â†’ GPTâ€‘2 forward
                 â†‘                       â†“
               write &lt;â€” pick best â—„â€” sieve rank</code></pre>

<h2>9 Â· Why It Feels Alive</h2>
<ul>
<li>Memoryâ€‘aware bias â‰ˆ stable persona.</li>
<li>Chronological context â‰ˆ story spine.</li>
<li>Sieve sampler â‰ˆ selfâ€‘editing.</li>
</ul>

<hr><p style="text-align:center"><em>Drain entropy, grow lotuses of awareness.</em> ðŸ’Ž</p>
</body></html>
